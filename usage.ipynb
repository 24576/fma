{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FMA: A Dataset For Music Analysis\n",
    "\n",
    "MichaÃ«l Defferrard, Kirell Benzi, Pierre Vandergheynst, Xavier Bresson, EPFL LTS2.\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Download dataset from <https://github.com/mdeff/fma>.\n",
    "2. Uncompress the archive, e.g. with `unzip fma_small.zip`.\n",
    "3. Load and play with the data in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import utils\n",
    "import librosa\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata and features.\n",
    "tracks = utils.load('tracks.csv')\n",
    "genres = utils.load('genres.csv')\n",
    "features = utils.load('features.csv')\n",
    "echonest = utils.load('echonest.csv')\n",
    "\n",
    "np.testing.assert_array_equal(features.index, tracks.index)\n",
    "\n",
    "# Directory where mp3 are stored.\n",
    "AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "\n",
    "tracks.shape, genres.shape, features.shape, echonest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Metadata\n",
    "\n",
    "The metadata table, a JSON file in the root directory of the archive, is composed of many colums:\n",
    "1. The index is the ID of the song, taken from the FMA, used as the name of the audio file.\n",
    "2. Per-track, per-album and per-artist metadata from the Free Music Archive website.\n",
    "3. Two columns to indicate the subset (small, medium, large, full) and the split (training, validation, test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.display(tracks['track'].head())\n",
    "ipd.display(tracks['album'].head())\n",
    "ipd.display(tracks['artist'].head())\n",
    "ipd.display(tracks['set'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} top-level genres'.format(len(genres['top_level'].unique())))\n",
    "genres.loc[genres['top_level'].unique()].sort_values('#tracks', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres.sort_values('#tracks').head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Features\n",
    "\n",
    "1. Features extracted from the audio for all tracks.\n",
    "2. For some tracks, data colected from the [Echonest](http://the.echonest.com/) API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{1} features for {0} tracks'.format(*features.shape))\n",
    "columns = ['mfcc', 'chroma_cens', 'tonnetz', 'spectral_contrast']\n",
    "columns.append(['spectral_centroid', 'spectral_bandwidth', 'spectral_rolloff'])\n",
    "columns.append(['rmse', 'zcr'])\n",
    "for column in columns:\n",
    "    ipd.display(features[column].head().style.format('{:.2f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{1} features for {0} tracks'.format(*echonest.shape))\n",
    "ipd.display(echonest['echonest', 'metadata'].head())\n",
    "ipd.display(echonest['echonest', 'audio_features'].head())\n",
    "ipd.display(echonest['echonest', 'social_features'].head())\n",
    "ipd.display(echonest['echonest', 'ranks'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.display(echonest['echonest', 'temporal_features'].head())\n",
    "x = echonest.loc[5, ('echonest', 'temporal_features')]\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Audio\n",
    "\n",
    "You can listen to an audio excerpt with the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = utils.get_audio_path(AUDIO_DIR, 2)\n",
    "print('File: {}'.format(filename))\n",
    "ipd.Audio(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use [librosa](https://github.com/librosa/librosa) to extract the raw waveform and compute audio features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, sr = librosa.load(filename)\n",
    "print('Duration: {:.2f}s, {} samples'.format(x.shape[0] / sr, x.size))\n",
    "ipd.display(ipd.Audio(data=x, rate=sr))\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(x)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "S, freqs, bins, im = plt.specgram(x, NFFT=1024, Fs=sr, noverlap=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Genre classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 From features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = tracks['set', 'subset'] <= 'small'\n",
    "\n",
    "train = tracks['set', 'split'] == 'training'\n",
    "val = tracks['set', 'split'] == 'validation'\n",
    "test = tracks['set', 'split'] == 'test'\n",
    "\n",
    "y_train = tracks.loc[small & train, ('track', 'genre_top')]\n",
    "y_test = tracks.loc[small & test, ('track', 'genre_top')]\n",
    "X_train = features.loc[small & train, 'mfcc']\n",
    "X_test = features.loc[small & test, 'mfcc']\n",
    "\n",
    "print('{} training examples, {} testing examples'.format(y_train.size, y_test.size))\n",
    "print('{} features, {} classes'.format(X_train.shape[1], np.unique(y_train).size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure training samples are shuffled.\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance.\n",
    "scaler = StandardScaler(copy=False)\n",
    "scaler.fit_transform(X_train)\n",
    "scaler.transform(X_test)\n",
    "\n",
    "# Support vector classification.\n",
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "print('Accuracy: {:.2%}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 From audio"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
